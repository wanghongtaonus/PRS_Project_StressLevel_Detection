{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d11e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "#from google.colab.patches import cv2_imshow\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b6f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9267f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg13_model(n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), input_shape=(64, 64, 1), padding='same', activation='relu',\n",
    "                     kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80aeb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg13_model(8)\n",
    "model.load_weights('./model_size64_Adagrad/model_40-0.80.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d59e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"test.mp4\")\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "\n",
    "# Read first frame.\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print('Cannot read video file')\n",
    "    \n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"neutral\", 1: \"happiness\", 2: \"surprise\", 3: \"sadness\", 4: \"anger\", 5: \"disgust\", 6: \"fear\", 7: \"contempt\"}\n",
    "\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    c = 0\n",
    "    b = 0\n",
    "    frame = cv2.addWeighted(frame, c, frame, 1-c, b)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.equalizeHist(gray, gray)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.15, minNeighbors=5, minSize=(5, 5),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img=cv2.resize(roi_gray,(64,64), interpolation=cv2.INTER_CUBIC)\n",
    "        input_data=np.array(cropped_img)\n",
    "        input_data = input_data.reshape(-1, 64, 64, 1)\n",
    "        prediction = model.predict(input_data)\n",
    "        #print(prediction)\n",
    "        maxindex1 = int(np.argmax(prediction))\n",
    "        promax1 = prediction[0,maxindex1]\n",
    "        prediction[0,maxindex1] = 0.0\n",
    "        maxindex2 = int(np.argmax(prediction))\n",
    "        promax2 = prediction[0,maxindex2]\n",
    "        #print(maxindex)\n",
    "        #if(maxindex1 == 0 and promax2 > 0.1):\n",
    "        #    maxindex = maxindex2\n",
    "        #else:\n",
    "        #    maxindex = maxindex1\n",
    "        #cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, emotion_dict[maxindex1] + \"{:.2f}\".format(promax1), (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, emotion_dict[maxindex2] + \"{:.2f}\".format(promax2), (x+40, y-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(704,576),interpolation = cv2.INTER_CUBIC))\n",
    "    #cv2.imshow('Video', cv2.resize(gray,(704,576),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not open video\n",
      "Cannot read video file\n"
     ]
    }
   ],
   "source": [
    "tracker = cv2.TrackerKCF_create()\n",
    "video = cv2.VideoCapture(\"test.mp4\")\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "\n",
    "# Read first frame.\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print('Cannot read video file')\n",
    "\n",
    "# Local computer, use mouse to select the target bounding box, press ESC to finish\n",
    "bbox = cv2.selectROI(frame, False)\n",
    "#bbox =  tuple([370, 260, 50, 50])\n",
    "ok = tracker.init(frame, bbox)\n",
    "\n",
    "frameIndex = 0\n",
    "frameTotal = 10\n",
    "while (1): # Specify the total number of frames displayed in Colab\n",
    "    frameIndex+=1\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # Update tracker\n",
    "    ok, bbox = tracker.update(frame)\n",
    "    # Draw bounding box\n",
    "    if ok:\n",
    "        # Tracking success\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        roi_gray = gray[int(bbox[1]):int(bbox[1] + bbox[3]),int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "        cropped_img=cv2.resize(roi_gray,(64,64), interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imshow(\"t\", cropped_img)\n",
    "        input_data=np.array(cropped_img)\n",
    "        input_data = input_data.reshape(-1, 64, 64, 1)\n",
    "        prediction = model.predict(input_data)\n",
    "        maxindex1 = int(np.argmax(prediction))\n",
    "        promax1 = prediction[0,maxindex1]\n",
    "        prediction[0,maxindex1] = 0.0\n",
    "        maxindex2 = int(np.argmax(prediction))\n",
    "        promax2 = prediction[0,maxindex2]\n",
    "        cv2.putText(frame, emotion_dict[maxindex1] + \"{:.2f}\".format(promax1), (0, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, emotion_dict[maxindex2] + \"{:.2f}\".format(promax2), (0, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else :\n",
    "        # Tracking failure\n",
    "        cv2.putText(frame, \"Tracking failure detected\", (10,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "    # Display tracker type on frame\n",
    "    cv2.putText(frame, \"Tracker, Frame \" + str(frameIndex), (10,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);\n",
    "\n",
    "    # Display result\n",
    "    cv2.imshow(\"Tracking\", frame) # local computer\n",
    "    #cv2_imshow(frame) # colab\n",
    "\n",
    "    # Exit if ESC pressed (for local computer)\n",
    "    k = cv2.waitKey(0) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902b77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
